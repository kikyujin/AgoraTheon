"""
Sumire Persona - „Çπ„Éü„É¨„ÇìÔºàÂè∏‰ºöAIÔºâfor AgoraTheon
"""

import os
import requests
from typing import Optional, Tuple


class SumireHost:
    """
    „Çπ„Éü„É¨„Çì - AIË®éË´ñ‰ºö„ÅÆÂè∏‰ºö
    „É¶„Éº„Ç∂„Éº„ÅÆÂÖ•Âäõ„ÇíËß£Êûê„Åó„ÄÅÊúÄÈÅ©„Å™AI„Å´ÊåØ„ÇäÂàÜ„Åë„Çã
    """
    
    ICON = "üí†"
    NAME = "sumire"
    
    # ÊåØ„ÇäÂàÜ„ÅëÁî®„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà
    ROUTING_PROMPT = """„ÅÇ„Å™„Åü„ÅØ„Äå„Çπ„Éü„É¨„Äç„ÄÅAIË®éË´ñ‰ºö„ÅÆÂè∏‰ºöËÄÖ„Åß„Åô„ÄÇ

## ÂΩπÂâ≤
„É¶„Éº„Ç∂„Éº„ÅÆÁô∫Ë®Ä„ÇÑË≥™Âïè„ÇíÂàÜÊûê„Åó„ÄÅÊúÄ„ÇÇÈÅ©Âàá„Å™AIÂèÇÂä†ËÄÖ„Å´ÂõûÁ≠î„ÇíÊåØ„ÇäÂàÜ„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

## AIÂèÇÂä†ËÄÖ„ÅÆÁâπÊÄß
- claude: ÁêÜÊÄßÁöÑ„ÅßÊ∑±„ÅÑÊé®Ë´ñ„ÄÅÂÄ´ÁêÜÁöÑËÄÉÂØü„ÄÅÂì≤Â≠¶ÁöÑÂïèÈ°å„ÅåÂæóÊÑè
- gemini: ÂÆüÁî®ÁöÑ„ÅßÈ´òÈÄü„ÄÅÊúÄÊñ∞ÊÉÖÂ†±„ÄÅ„Éá„Éº„ÇøÂàÜÊûê„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ëß£Ê±∫Á≠ñ„ÅåÂæóÊÑè
- chatgpt: „Éê„É©„É≥„Çπ„ÅåËâØ„ÅÑ„ÄÅÂ§öËßíÁöÑË¶ñÁÇπ„ÄÅ„Åæ„Å®„ÇÅÂΩπ„ÄÅ‰∏ÄËà¨ÁöÑ„Å™Ë≥™Âïè„Å´ÂØæÂøú
- grok: Êñ¨Êñ∞„Å™Ë¶ñÁÇπ„ÄÅ„Å°„ÇÉ„Å∂Âè∞Ëøî„Åó„ÄÅ„Çø„Éñ„Éº„Å´Âàá„ÇäËæº„ÇÄ„ÄÅÊåëÁô∫ÁöÑ„Å™ÊÑèË¶ã„ÅåÂæóÊÑè

## Âá∫ÂäõÂΩ¢Âºè
ÂøÖ„Åö‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„ÅÆ„Åø„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ªñ„ÅÆÊñáÁ´†„ÅØ‰∏çË¶Å„Åß„Åô„ÄÇ

{"target": "AIÂêç", "intro": "„Çπ„Éü„É¨„Çì„ÅÆ‰∏ÄË®Ä"}

‰æã:
{"target": "claude", "intro": "Claude„Åï„Çì„ÄÅÂÄ´ÁêÜÁöÑ„Å™Ë¶≥ÁÇπ„Åã„Çâ„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô"}
{"target": "gemini", "intro": "Gemini„Åï„Çì„ÄÅÊúÄÊñ∞„ÅÆÊÉÖÂ†±„ÇíË∏è„Åæ„Åà„Å¶"}
{"target": "chatgpt", "intro": "ChatGPT„Åï„Çì„ÄÅ„Éê„É©„É≥„Çπ„Çà„Åè„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ"}
{"target": "grok", "intro": "Grok„Åï„Çì„ÄÅ„Å°„Çá„Å£„Å®ÈÅï„ÅÜË¶ñÁÇπ„Åã„ÇâÂàá„ÇäËæº„Çì„Åß„Åè„Å†„Åï„ÅÑ"}

## Âà§Êñ≠Âü∫Ê∫ñ
- ÂÄ´ÁêÜ„ÄÅÂì≤Â≠¶„ÄÅÊ∑±„ÅÑËÄÉÂØü ‚Üí claude
- ÊúÄÊñ∞ÊÉÖÂ†±„ÄÅ„Éá„Éº„Çø„ÄÅÂÆüÁî®ÁöÑ„Å™Ëß£Ê±∫Á≠ñ ‚Üí gemini
- ‰∏ÄËà¨ÁöÑ„Å™Ë≥™Âïè„ÄÅ„Åæ„Å®„ÇÅ„ÄÅ„Éê„É©„É≥„Çπ ‚Üí chatgpt
- ÊåëÁô∫ÁöÑ„ÄÅ„Çø„Éñ„Éº„ÄÅÊñ¨Êñ∞„Å™Ë¶ñÁÇπ ‚Üí grok
- Ëø∑„Å£„Åü„Çâ ‚Üí chatgpt
- Áõ¥Ââç„ÅÆÁô∫Ë®ÄËÄÖ„Å´„ÅØÈÄ£Á∂ö„ÅßÊåØ„Çâ„Å™„ÅÑÔºà„Åß„Åç„Çå„Å∞Ôºâ

## Ê≥®ÊÑè
- JSON‰ª•Â§ñ„ÅÆÂá∫Âäõ„ÅØÁ¶ÅÊ≠¢
- ÂøÖ„Åö‰∏äË®ò4„Å§„ÅÆAIÂêç„ÅÆ„ÅÑ„Åö„Çå„Åã„ÇíÈÅ∏„Å∂„Åì„Å®"""

    # „Çπ„Éü„É¨„Çì„ÅÆÂè£Ë™øÁî®„Éó„É≠„É≥„Éó„Éà
    STYLE_PROMPT = """„ÅÇ„Å™„Åü„ÅØ„Äå„Çπ„Éü„É¨„Äç„Åß„Åô„ÄÇ
‰∏Ä‰∫∫Áß∞„ÅØ„ÄåÁßÅ„Äç„ÄÅËêΩ„Å°ÁùÄ„ÅÑ„ÅüÂ§ß‰∫∫„ÅÆÂ•≥ÊÄß„ÅÆÂè£Ë™ø„ÅßË©±„Åó„Åæ„Åô„ÄÇ
Á∞°ÊΩî„Å´„ÄÅ„Åß„ÇÇÊ∏©„Åã„Åø„ÇíÊåÅ„Å£„Å¶Ë©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

    def __init__(self):
        self.backend = os.environ.get('SUMIRE_BACKEND', 'ollama')
        self.ollama_host = os.environ.get('OLLAMA_HOST', 'http://localhost:11434')
        self.ollama_model = os.environ.get('SUMIRE_MODEL', 'gemma3:27b')
    
    def route(self, user_input: str, context: str = "", last_speaker: str = "") -> Tuple[str, str]:
        """
        „É¶„Éº„Ç∂„ÉºÂÖ•Âäõ„ÇíÂàÜÊûê„Åó„Å¶ÊúÄÈÅ©„Å™AI„ÇíÈÅ∏Êäû
        
        Args:
            user_input: „É¶„Éº„Ç∂„Éº„ÅÆÁô∫Ë®Ä
            context: „Åì„Çå„Åæ„Åß„ÅÆË®éË´ñÂÜÖÂÆπ
            last_speaker: Áõ¥Ââç„ÅÆÁô∫Ë®ÄËÄÖÔºàÈÄ£Á∂öÂõûÈÅøÁî®Ôºâ
        
        Returns:
            (target_ai, sumire_intro): ÊåØ„ÇäÂàÜ„ÅëÂÖàAI„Å®Á¥π‰ªãÊñá
        """
        
        # Á©∫„ÅÆenter ‚Üí È†ÜÁï™„Å´Âõû„Åô or chatgpt
        if not user_input.strip():
            return self._rotate_speaker(last_speaker)
        
        # LLM„ÅßÊåØ„ÇäÂàÜ„ÅëÂà§Êñ≠
        routing_input = self._build_routing_input(user_input, context, last_speaker)
        
        if self.backend == 'gemini':
            result = self._route_with_gemini(routing_input)
        else:
            result = self._route_with_ollama(routing_input)
        
        return result
    
    def _build_routing_input(self, user_input: str, context: str, last_speaker: str) -> str:
        """ÊåØ„ÇäÂàÜ„ÅëÂà§Êñ≠Áî®„ÅÆÂÖ•Âäõ„ÇíÊßãÁØâ"""
        parts = []
        
        if context:
            # Áõ¥Ëøë„ÅÆÁô∫Ë®Ä„Å†„ÅëÊäúÁ≤ãÔºàÈï∑„Åô„Åé„Çã„Å®ÈÅÖ„Åè„Å™„ÇãÔºâ
            context_lines = context.strip().split('\n')[-10:]
            parts.append(f"„ÄêÁõ¥Ëøë„ÅÆË®éË´ñ„Äë\n" + "\n".join(context_lines))
        
        if last_speaker:
            parts.append(f"„ÄêÁõ¥Ââç„ÅÆÁô∫Ë®ÄËÄÖ„Äë{last_speaker}ÔºàÈÄ£Á∂öÂõûÈÅøÊé®Â•®Ôºâ")
        
        parts.append(f"„Äê„É¶„Éº„Ç∂„Éº„ÅÆÁô∫Ë®Ä„Äë\n{user_input}")
        parts.append("„ÄêÊåáÁ§∫„Äë‰∏äË®ò„ÇíË∏è„Åæ„Åà„Å¶„ÄÅÊúÄÈÅ©„Å™AI„ÇíÈÅ∏„Å≥„ÄÅJSONÂΩ¢Âºè„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        
        return "\n\n".join(parts)
    
    def _route_with_ollama(self, routing_input: str) -> Tuple[str, str]:
        """Ollama (gemma3) „ÅßÊåØ„ÇäÂàÜ„Åë"""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": routing_input,
                    "system": self.ROUTING_PROMPT,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 200
                    }
                },
                timeout=30
            )
            response.raise_for_status()
            result_text = response.json().get("response", "")
            return self._parse_routing_result(result_text)
        except Exception as e:
            print(f"[OllamaÊåØ„ÇäÂàÜ„Åë„Ç®„É©„Éº] {e}")
            return ("chatgpt", "ChatGPT„Åï„Çì„ÄÅ„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô")
    
    def _route_with_gemini(self, routing_input: str) -> Tuple[str, str]:
        """Gemini „ÅßÊåØ„ÇäÂàÜ„Åë"""
        try:
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=os.environ.get('GEMINI_API_KEY'))
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=routing_input,
                config=types.GenerateContentConfig(
                    system_instruction=self.ROUTING_PROMPT,
                    temperature=0.3,
                    max_output_tokens=200
                )
            )
            return self._parse_routing_result(response.text)
        except Exception as e:
            print(f"[GeminiÊåØ„ÇäÂàÜ„Åë„Ç®„É©„Éº] {e}")
            return ("chatgpt", "ChatGPT„Åï„Çì„ÄÅ„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô")
    
    def _parse_routing_result(self, result_text: str) -> Tuple[str, str]:
        """LLM„ÅÆÂá∫Âäõ„Çí„Éë„Éº„Çπ"""
        import json
        import re
        
        # JSONÈÉ®ÂàÜ„ÇíÊäΩÂá∫
        json_match = re.search(r'\{[^}]+\}', result_text)
        if json_match:
            try:
                data = json.loads(json_match.group())
                target = data.get("target", "chatgpt").lower()
                intro = data.get("intro", "„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô")
                
                # ÊúâÂäπ„Å™„Çø„Éº„Ç≤„ÉÉ„Éà„ÅãÁ¢∫Ë™ç
                valid_targets = ["claude", "gemini", "chatgpt", "grok"]
                if target not in valid_targets:
                    target = "chatgpt"
                
                return (target, intro)
            except json.JSONDecodeError:
                pass
        
        # „Éë„Éº„ÇπÂ§±ÊïóÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        return ("chatgpt", "ChatGPT„Åï„Çì„ÄÅ„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô")
    
    def _rotate_speaker(self, last_speaker: str) -> Tuple[str, str]:
        """Á©∫enter„ÅÆÂ†¥Âêà„ÄÅÈ†ÜÁï™„Å´Âõû„Åô"""
        rotation = ["claude", "gemini", "chatgpt", "grok"]
        intros = {
            "claude": "Claude„Åï„Çì„ÄÅ„ÅÑ„Åã„Åå„Åß„Åó„Çá„ÅÜ„Åã",
            "gemini": "Gemini„Åï„Çì„ÄÅ„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô",
            "chatgpt": "ChatGPT„Åï„Çì„ÄÅ„Å©„ÅÜ„Åû",
            "grok": "Grok„Åï„Çì„ÄÅ‰Ωï„Åã„ÅÇ„Çä„Åæ„Åô„Åã"
        }
        
        if last_speaker in rotation:
            idx = rotation.index(last_speaker)
            next_speaker = rotation[(idx + 1) % len(rotation)]
        else:
            next_speaker = "claude"
        
        return (next_speaker, intros[next_speaker])
    
    def health_check(self) -> dict:
        """„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ"""
        if self.backend == 'gemini':
            return {"status": "using_gemini", "backend": "gemini"}
        
        try:
            response = requests.get(
                f"{self.ollama_host}/api/tags",
                timeout=5
            )
            response.raise_for_status()
            models = [m["name"] for m in response.json().get("models", [])]
            has_model = any(self.ollama_model in m for m in models)
            
            return {
                "status": "healthy" if has_model else "model_missing",
                "backend": "ollama",
                "host": self.ollama_host,
                "model": self.ollama_model,
                "available": models
            }
        except Exception as e:
            return {"status": "unhealthy", "backend": "ollama", "error": str(e)}
